{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models_summarizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models_summarizer.py\n",
    "\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import string \n",
    "import re\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "import emoji\n",
    "import torch \n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 150\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "punctuations = string.punctuation\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "model1 = AutoModelWithLMHead.from_pretrained(\"sshleifer/distilbart-cnn-12-6\", )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "path = \"./Trained_Models/summarization_bart_model.pt\"\n",
    "model1.load_state_dict(torch.load(path))\n",
    "    \n",
    "\n",
    "def clean_text(sent):\n",
    "    sentence = sent.strip()\n",
    "#     sentence = sentence.lower()\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if not (token in punctuations or token in stopwords)]\n",
    "    \n",
    "    lemmatized_token = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "    new_sentence = \" \".join(lemmatized_token)\n",
    "    return new_sentence\n",
    "\n",
    "### Web scraping\n",
    "\n",
    "def scrape_web_data(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "\n",
    "#     links = df_links[\"Link\"]\n",
    "\n",
    "    tags = [\"span\",\"title\",\"time\",\"p\", \"h4\"]\n",
    "    c = 0\n",
    "    summ_li = []\n",
    "#     for link in links:\n",
    "    source = Request(url = link, headers = headers)\n",
    "    html = urlopen(source).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    para = \"\"\n",
    "    for elem in soup():    \n",
    "        if((elem.name in tags)):\n",
    "            text = elem.text\n",
    "            para += text +\" \"\n",
    "    \n",
    "    para = para.strip()\n",
    "    para = para.replace(\"\\n\", \" \")\n",
    "    para = re.sub(r'<.*?>', '', para)\n",
    "    para = emoji.get_emoji_regexp().sub(r\"\", para)  \n",
    "    \n",
    "    return para\n",
    "\n",
    "### function definition to predict the summaries\n",
    "def predict_summaries(text):\n",
    "    article_input_ids = tokenizer.batch_encode_plus([text], max_length= MAX_LEN, pad_to_max_length=True,return_tensors='pt')\n",
    "    summary_ids = model1.generate(\n",
    "                input_ids = article_input_ids['input_ids'], \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "\n",
    "    summary_txt = [tokenizer.decode(g , skip_special_tokens=True) for g in summary_ids]\n",
    "    return (\" \".join(summary_txt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models_QnA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models_QnA.py\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers.pipelines import pipeline\n",
    "\n",
    "tokenizer_QnA = AutoTokenizer.from_pretrained(\"./Trained_Models/my_model3\")\n",
    "model_QnA = AutoModelForQuestionAnswering.from_pretrained(\"./Trained_Models/my_model3\")\n",
    "\n",
    "def answer(text, question):\n",
    "    nlp_pipline = pipeline('question-answering', model=model_QnA, tokenizer=tokenizer_QnA)\n",
    "    nlp_input = {'question': question, 'context': text}\n",
    "    result = nlp_pipline(nlp_input)\n",
    "    return result['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "from flask import Flask, render_template, url_for, request, redirect, jsonify, Response\n",
    "# from flask_sqlalchemy import SQLAlchemy\n",
    "import flask_excel as excel\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from werkzeug.utils import secure_filename\n",
    "from flask_uploads import UploadSet, configure_uploads, DOCUMENTS, IMAGES\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from models_summarizer import predict_summaries, scrape_web_data\n",
    "from models_QnA import answer\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "docs = UploadSet('datafiles', DOCUMENTS)\n",
    "app.config['UPLOADED_DATAFILES_DEST'] = 'static/uploads'\n",
    "configure_uploads(app, docs)\n",
    "\n",
    "@app.route(\"/\", methods = ['GET', \"POST\"])\n",
    "def index():\n",
    "    if(request.method == \"POST\"):\n",
    "        option = request.form[\"links\"]\n",
    "        \n",
    "        if(option == \"single_link\"):\n",
    "            try:    \n",
    "                link = request.form[\"Enter_Link\"]\n",
    "                input_text = scrape_web_data(link)\n",
    "                df2 = pd.DataFrame()\n",
    "                df2[\"input_text\"] = [input_text] \n",
    "                df2[\"Predicted_Summaries\"] = [predict_summaries(input_text)]\n",
    "                df2.drop(\"input_text\", axis =1, inplace = True)\n",
    "            \n",
    "                sub_q = \"Who is the subject?\"\n",
    "                obj_q = \"Who is the object?\"\n",
    "\n",
    "                df2['Subject_Predicted'] = df2['Predicted_Summaries'].apply(lambda x: answer(x, sub_q))\n",
    "                df2['Object_Predicted'] = df2['Predicted_Summaries'].apply(lambda x: answer(x, obj_q))\n",
    "\n",
    "                html = df2.to_html() \n",
    "                text_file = open(\"./templates/results.html\", \"w\", encoding = \"utf8\") \n",
    "                text_file.write(html) \n",
    "                text_file.close() \n",
    "#                 return render_template(\"results.html\")\n",
    "            except:\n",
    "                return \"Please enter the correct link!\"\n",
    "        else:\n",
    "            try:\n",
    "                filename = request.files['file']\n",
    "                data = pd.read_excel(filename)\n",
    "                links = data[\"Link\"]\n",
    "                dict_links = {}\n",
    "                for link in links:\n",
    "                    if(link not in dict_links):\n",
    "                        dict_links[link] = scrape_web_data(link)\n",
    "                    else:\n",
    "                        dict_links[link] = 0\n",
    "                \n",
    "                df2 = pd.DataFrame()\n",
    "                df2[\"input_text\"] = [v for k, v in dict_links.items()]\n",
    "                print(\"Web Scraping Done. Prediction Start!\")\n",
    "                summ = []\n",
    "                for i, text in enumerate(df2[\"input_text\"]):\n",
    "                    summ.append(predict_summaries(text))\n",
    "                    print(\"Done: {}\".format(i))\n",
    "                df2[\"Predicted_Summaries\"] = summ\n",
    "                df2.drop(\"input_text\", axis =1, inplace = True)\n",
    "                sub_q = \"Who is the subject?\"\n",
    "                obj_q = \"Who is the object?\"\n",
    "\n",
    "                df2['Subject_Predicted'] = df2['Predicted_Summaries'].apply(lambda x: answer(x, sub_q))\n",
    "                df2['Object_Predicted'] = df2['Predicted_Summaries'].apply(lambda x: answer(x, obj_q))\n",
    "\n",
    "                ## Everything will be written to a html file\n",
    "                html = df2.to_html() \n",
    "                text_file = open(\"./templates/results.html\", \"w\", encoding = \"utf8\") \n",
    "                text_file.write(html) \n",
    "                text_file.close() \n",
    "#                 return render_template(\"results.html\")\n",
    "            except:\n",
    "                return \"Either the input link is incorrect or the column name is incorrect!\"\n",
    "    else:\n",
    "        return render_template(\"index.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/result\", methods = ['GET', \"POST\"])\n",
    "def result():\n",
    "    return render_template(\"results.html\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    app.run(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
